# PaJpRAG_system

---

```markdown
# RAG (Retrieval Augmented Generation) デモ

このプロジェクトは、Retrieval Augmented Generation (RAG) システムのデモンストレーションです。
大規模言語モデル (LLM) の「幻覚」問題や情報鮮度の課題を解決するため、外部の知識ベースから関連情報を検索し、それを基に回答を生成します。

## 特徴

* **RAGシステムの実装:** SentenceTransformer を用いたセマンティック検索と、FAISS を用いた高速なベクトル検索を統合しています。
* **日本語LLMの利用:** `rinna/japanese-gpt-neox-3.6b-instruction-sft` モデルを推論に使用しています。
* **カスタム知識ベース:** ユーザーが独自のJSONファイルをアップロードし、RAGシステムの知識ベースを動的に更新できます。
* **Gradio UI:** 直感的で使いやすいWebインターフェースを提供し、RAGシステムの動作を視覚的に確認できます。
* **永続化機能:** アップロードされた文書と構築されたインデックスは、アプリケーションが再起動しても維持されます。

## システム構成

* **`app.py`**: Gradio UI の定義と、RAGSystem および LLM パイプラインの統合を管理するメインアプリケーションファイルです。
* **`ragsys03.py`**: RAGシステムのコアロジック（文書のエンベディング、FAISSインデックスの構築と検索、文書管理、永続化）をカプセル化したモジュールです。
* **`rag_data/`**: RAGシステムが生成するFAISSインデックスファイル、ロードされた文書データ、およびメタデータが保存されるディレクトリです。


.
├── app.py
├── ragsys03.py
├── requirements.txt
└── rag_data/  (RAGシステムが生成するインデックスや文書の保存先)
    └── (日付_UUID)/
        ├── faiss_index.bin
        ├── documents.json
        └── metadata.json
```

## 必要なライブラリ

以下のライブラリが必要です。`requirements.txt` ファイルに記載されています。

```
torch
transformers
sentence-transformers
faiss-cpu
gradio
numpy
sentencepiece
accelerate
```

インストールするには、以下のコマンドを実行します。

```bash
pip install -r requirements.txt
```

**注:** GPU環境で高速化したい場合は、`faiss-cpu` の代わりに `faiss-gpu` をインストールしてください。

## 実行方法

1.  GitHubリポジトリをクローンするか、Hugging Face Space のファイルをダウンロードします。
2.  `app.py` と `ragsys03.py` を同じディレクトリに配置します。
3.  上記の「必要なライブラリ」をインストールします。
4.  ターミナルで以下のコマンドを実行し、アプリケーションを起動します。

    ```bash
    python app.py
    ```

5.  表示されたURL（通常は `http://127.0.0.1:7860` または Hugging Face Spaces のURL）をブラウザで開きます。

## 使用方法 (Gradio UI)

1.  **「文書管理」タブに移動します。**
2.  **「文書JSONファイルをアップロード」:**
    * `documents` というキーに文字列のリストを持つJSONファイルを選択してアップロードします。
    * 例:

        ```json
        {
          "documents": [
            "RAG（Retrieval Augmented Generation）は、大規模言語モデルの課題、特に幻覚や情報鮮度の問題を解決するために考案された強力なAIフレームワークです。",
            "LLMの幻覚（Hallucination）は、大規模言語モデルが事実と異なる情報を生成してしまう問題です。"
          ]
        }
        ```

    * アップロードが完了すると、ステータスが表示されます。
3.  **「インデックスを構築」ボタンをクリックします。**
    * アップロードされた文書から検索インデックスが構築されます。この処理には時間がかかる場合があります。
    * 構築が完了すると、インデックスの統計情報が表示されます。
4.  **「RAG質問」タブに移動します。**
5.  **「質問を入力してください」:** 質問を入力します。
6.  **「取得文書数 (top_k)」と「類似度閾値」:** 必要に応じてスライダーを調整します。
7.  **「質問を送信」ボタンをクリックします。**
    * RAGシステムが関連文書を検索し、LLMがそれに基づいて回答を生成します。
    * 「LLMの回答」と「検索された関連文書」が表示されます。

## 注意事項

* このデモはCPU環境でも動作しますが、`rinna/japanese-gpt-neox-3.6b-instruction-sft` モデルは比較的大規模であるため、応答に時間がかかる場合があります。
* より高速な応答が必要な場合は、GPU環境での実行を推奨します。
* アップロードする文書の内容は、公開に適したものであることを確認してください。プライバシーや機密情報を含む文書はアップロードしないでください。
* LLMの回答は、提供された文書とモデルの知識に基づいています。常に正確であるとは限らず、「幻覚」を完全に排除するものではありません。

## 貢献

このプロジェクトはデモンストレーションを目的としていますが、改善提案やバグ報告は歓迎します。
```
